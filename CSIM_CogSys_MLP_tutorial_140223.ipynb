{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drawVT/RSSHub/blob/master/CSIM_CogSys_MLP_tutorial_140223.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [30860] Cognitive Systems: Theories and Models\n",
        "## Week 6 - Multi Layer Networks Tutorial\n",
        "\n",
        "\n",
        "Authors: R. Zucca, G. Maffei, and A. F. Amil"
      ],
      "metadata": {
        "id": "4oNNPpzBdxcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part I: Implementing a 3-layer neural network from scratch"
      ],
      "metadata": {
        "id": "YbqHiPMUjwPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import the necessary  modules"
      ],
      "metadata": {
        "id": "s4_6PpmWdxfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sb\n",
        "import numpy.random as random\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Display plots inline and change default figure size\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (5.0, 5.0)"
      ],
      "metadata": {
        "id": "XbNxwmDodxrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating a toy dataset\n",
        "\n",
        "We import a dataset from the ones available in the *scikit-learn* package (www.scikit-learn.org). \n",
        "\n",
        "Scikit-learn provides many predefined datasets that can be used to test your models.\n",
        "\n",
        "\n",
        "Some examples: \n",
        "\n",
        "- `sklearn.datasets.load_boston` load and return the boston house-prices dataset (regression).\n",
        "\n",
        "- `sklearn.datasets.load_breast_cancer` Load and return the breast cancer wisconsin dataset (classification).\n",
        "\n",
        "- `sklearn.datasets.make_circles` Make a large circle containing a smaller circle in 2d (clustering and classification)\n",
        "\n",
        "Let's use the <b>make_moons</b> one which produces Gaussian data with a spherical decision boundary for binary classification."
      ],
      "metadata": {
        "id": "C2FJYoQEeKLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed generator to reproduce the same results\n",
        "random_state = 0\n",
        "\n",
        "# Generate a dataset of 200 points from a Gaussian distribution with standar deviation equal to 0.2\n",
        "# Input:\n",
        "#     n_points: We generate 200 points\n",
        "#     noise: Standard deviation of Gaussian noise added to the data.\n",
        "# Output:\n",
        "#     X: training dataset\n",
        "#     y: target labels\n",
        "\n",
        "\n",
        "n_samples = 200\n",
        "noise     = 0.2\n",
        "\n",
        "X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
        "\n",
        "# Check the dimensionality of the generated dataset\n",
        "print('X (samples x features): %d x %d' % (X.shape[0], X.shape[1]))\n",
        "print('y (labels): %d ' % y.shape)\n",
        "\n",
        "# Define some colormaps\n",
        "cm = plt.cm.RdBu\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=cm_bright)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ned-SlhFdyFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have two classes of datapoints separated by a <b>non linear</b> boundary."
      ],
      "metadata": {
        "id": "9gN2LAGUe0Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is used to plot the decision boundary\n",
        "\n",
        "def plot_decision_boundary(pred_func):\n",
        "    # Set min and max values and give it some padding\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    # Step size in the meshgrid\n",
        "    h = 0.01\n",
        "    # Generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \n",
        "                         np.arange(y_min, y_max, h))\n",
        "    # Predict the function value for the whole gid\n",
        "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])   # ravel function returns a contiguous flattened array\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # Plot the contour and training examples\n",
        "    plt.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)"
      ],
      "metadata": {
        "id": "QpM1EywadyJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### As we have already seen, a SLP is not able to correctly classify such a dataset\n",
        "\n",
        "Minksy and Papert showed that a two layer feed-forward network can overcome many restrictions of single layer networks, but did not present a solution to the problem of how to adjust the weights from input to hidden units.\n",
        "\n",
        "An answer to this question was provided by Rumelhart, Hinton and Williams (1986):\n",
        "- the central idea is that errors for the units of the hidden layer are determined by back-propagating the errors of the units of the output layer."
      ],
      "metadata": {
        "id": "uho5wa1TfHnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron(training_set, target_vector, stopAt=1000, w=np.zeros(3)):\n",
        "    '''\n",
        "        A very simple implementation of the perceptron algorithm for two dimensional data.\n",
        "        Input: \n",
        "            training_set  : Data points, an Nx2 vector. \n",
        "            target_vector : Classification of the previous data points, an Nx1 vector. \n",
        "            stopAt  : Maximum number of iterations (optional).\n",
        "            w             : Initial vector of weights (by default they are initialized with zeros)\n",
        "            \n",
        "        Output: \n",
        "            w : Weight vector (i.e., parameters of the best linear separator, y = ax+b)\n",
        "'''\n",
        "    # Number of training samples\n",
        "    N = training_set.shape[0]\n",
        "    \n",
        "    # Activation function\n",
        "    # Expanded formula\n",
        "    f = lambda x: np.sign(w[0]+w[1]*x[0]+w[2]*x[1])\n",
        "\n",
        "    ########################\n",
        "    # Learning loop\n",
        "    ########################\n",
        "    i = 0  # counter for examples\n",
        "    c = 1\n",
        "    for _ in range(stopAt):\n",
        "        \n",
        "        i = np.random.randint(N)\n",
        "        # Calculate prediction\n",
        "        y_hat = f(training_set[i,:])\n",
        "        \n",
        "        # Error correction\n",
        "        if(target_vector[i] != y_hat): \n",
        "             error = target_vector[i] - y_hat\n",
        "             w[0] = w[0] + error # correct the bias term\n",
        "             w[1] = w[1] + error*training_set[i,0] # correct the first weight\n",
        "             w[2] = w[2] + error*training_set[i,1] # correct the second weight\n",
        "        i+=1\n",
        "        if np.mod(i,N)==0:\n",
        "            #print 'Training epoch:', c \n",
        "            c+=1\n",
        "            i = 0\n",
        "    return w;"
      ],
      "metadata": {
        "id": "Qsf30CuGfHxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xn = X/np.std(X, axis=0)\n",
        "W = perceptron(X, y, stopAt=10000)"
      ],
      "metadata": {
        "id": "Rl0SyEIpgCxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale the weights to draw the boundary line\n",
        "b_new = -W[0]/W[2]\n",
        "a_new = -W[1]/W[2]\n",
        "x = np.linspace(-2, 3, 50)\n",
        "f = lambda x: a_new * x + b_new\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(Xn[:,0], Xn[:,1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "plt.plot(x,f(x),'b--',label = 'Perceptron solution')\n",
        "plt.ylim(-2,3)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GS8Z6bjbgC0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron"
      ],
      "metadata": {
        "id": "fhNX2fl9gBHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A multilayer perceptron (MLP) is a deep, artificial neural network. It is composed of more than one perceptron. \n",
        "\n",
        "They are composed of an input layer to receive the signal, an output layer that makes a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP. MLPs with one hidden layer are capable of approximating any continuous function.\n",
        "\n",
        "\n",
        "We build a 3-layer neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of the data, i.e. 2.\n",
        "\n",
        "The number of nodes in the output layer is determined by the number of classes we have, i.e.,  1 unit for a binary classification problem, 2 units for multiclass problems. \n",
        "\n",
        "The input to the network will be x- and y- coordinates and its output will be the predicted class. Depending on the activation function we can have a fixed or continuous value (representing the probability to belong to a class)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1J0HNZvnKs6koL_wbixsUfqClI2nFe51_\" width=\"600\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1w6INnIehjB56CiICmW9qaIEv7V-HxA00\" width=\"600\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1pbAXUkarhKL1MdSM3Xnpukq3zuSzni7q\" width=\"600\">"
      ],
      "metadata": {
        "id": "HmTKqp_KglX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hidden layer\n",
        "The dimensionality of the hidden layer depends on the specific problem you are dealing with.\n",
        "\n",
        "More nodes can deal with more complex functions, but at the cost of more computations and the possibility to overfit the data.  Overfitting occurs when a model with high capacity fits the noise in the data instead of the (assumed) underlying relationship.\n",
        "\n",
        "It has been shown that only one layer of hidden units suffices to approximate any function, provided the activation function of the hidden units is non linear."
      ],
      "metadata": {
        "id": "bVTZBFU6g-QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting problem\n",
        "### The problem at hands\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XOjpMKazYKLtlraUxAsK__DbkUMRTD3l\" width=\"500\">\n",
        "\n",
        "### A complex model\n",
        "<img src=\"https://drive.google.com/uc?id=1fpT-VpnkrZYG2miRvKNczOaM_rBSVMsH\" width=\"500\">\n",
        "\n",
        "### The true (much simpler) model\n",
        "<img src=\"https://drive.google.com/uc?id=1dlQBgmH6mGybMvnEJZZy_SJfOxygEx7w\" width=\"500\">\n",
        "\n",
        "### How overfitting affects prediction\n",
        "<img src=\"https://drive.google.com/uc?id=1dQioFSsnxX-N8OLy9_pAlfN2QAJ5FjK9\" width=\"600\">"
      ],
      "metadata": {
        "id": "pP58ULTYhC60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The activation function\n",
        "Choose an activation function for the hidden layer (i.e, linear rectifier, sigmoid, etc)."
      ],
      "metadata": {
        "id": "UiXddAjxhFOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization: learning the parameters that minimize the loss function\n",
        "\n",
        "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that *minimize the error* on our training data. \n",
        "\n",
        "The amount of error is calculated through the *loss function* (loss for prediction with respect to true labels). The loss function can be considered as a quality measure"
      ],
      "metadata": {
        "id": "OwkNtFekhIss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back-propagation\n",
        "\n",
        "When a learning pattern is clamped  the activation values are propagated to the output units  and the actual network output is compared with the desired output values  we usually end up with an error in each of the output units.\n",
        "\n",
        "Let's call this error $e_o$ for a particular output unit $o$.  We have to bring $e_o$ to $0$. \n",
        "\n",
        "**Step 1**\n",
        "The simplest way to achieve this is the **greedy method**: we strive to change the connections in such a way that, next time arounds, the error $e_o$ will be zero for this particular pattern. In order to reduce the error we have to adapt its incoming weights according to \n",
        "\n",
        "$$ \\delta w_{ho} = (d_o - y_o ) y_h$$\n",
        "\n",
        "**Step 2**\n",
        "To adapt the weights from input to hidden units, we again want to apply the delta rule. In this case, we don't have a value for $\\delta$ in the hidden units. This is solved by the so called **chain rule**:\n",
        "\n",
        "- the error of a output unit $o$ is distributed to all the hidden units that it is connected to, weighted by this connection or differently said:\n",
        "- a hidden unit $h$ receives a delta from each output unit $o$ equal to the delta of that output unit weighted with the weight of the connection between those units. The activation function of the hidden unit is then applied to the delta.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1uBlf2KH73GTnXQf7WZJSum-LC1uACA2L\" width=\"600\">\n",
        "\n",
        "#### How BP works\n",
        "**Phase 1**\n",
        "- The input **x** is presented and propagated forward through the network to compute the output value **y** for each unit.\n",
        "- The output is compared with the desired value **d** resulting in an error signal $\\delta$ for each output unit.\n",
        "\n",
        "**Phase 2**\n",
        "- The error is passed back to each unit and the weight changes are calculated.\n",
        "\n",
        "When using a sigmoid function:\n",
        "- The weight of a connection is adjusted by an amount prop ortional to the pro duct of an error signal on the unit k receiving the input and the output of the unit j sending this signal along the connection\n",
        "\n",
        "$$\\Delta_p w_{jk} = \\lambda \\delta_k y_j$$\n",
        "\n",
        "- If the unit is an output unit the error is given by:\n",
        "\n",
        "$$\\delta_o = (\\delta_o - y_o) F(s_o)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$y = F(s) = 1/(1+e^{-s})$$\n",
        "\n",
        "in this case:\n",
        "\n",
        "$$F(s) = y(1-y)$$\n",
        "\n",
        "and the error is:\n",
        "\n",
        "$$\\delta_o = (d_o -y_o ) y_o (1-y_o)$$\n",
        "\n",
        "The error signal of a hidden unit is determined recurisvely in terms of error signals of the units to which it directly connects and the weights of those connections:\n",
        "\n",
        "$$\\delta_h = y_h (1-y_h) \\sum \\delta_o w_{ho}$$\n",
        "\n",
        "**Learning and momentum**\n",
        "Learning requires that the chnage in weight is porportional to $dE / dw$. \n",
        "\n",
        "The proportionality constant (allowing for infinitesimal steps) is the learning rate $\\lambda$. To avoid oscillations the change is made dependent on the past weight change by adding a momentum term.\n",
        "\n",
        "\n",
        "**Learning per pattern**\n",
        "The order of pattern presentation is important. To avoid the network to focus on the first few patterns the order of presentation is shuffled. "
      ],
      "metadata": {
        "id": "Eqiy4aD2hK8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limits of BP\n",
        "- Long training process\n",
        "- Network paralysis (too high values) and local minima (the network get trapped in a local minimum that is not the optimal solution)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=13SFWpk1fgWL5PSXVfq30q_tuRI_35sRi\" width=\"600\">"
      ],
      "metadata": {
        "id": "MRpcNcfQhPvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How good are multi-layer FF networks?\n",
        "The approximation of a network is not perfect. It depends on:\n",
        "- The learning algorithm and number of iterations.  This determines how good the error on the training set is minimized\n",
        "- The number of learning samples.  This determines how good the training samples represent the actual function\n",
        "- The number of hidden units.  This determines the  expressive power  of the network. For  smooth  functions only a few number of hidden units are needed  for wildly  fluctuating functions more hidden units will be needed."
      ],
      "metadata": {
        "id": "jSQKPwdHhU46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An example: How our network makes predictions\n",
        "\n",
        "If $x$ is the 2D input to our network then we calculate our output $\\hat{y}$ (also two-dimensional) as follows:"
      ],
      "metadata": {
        "id": "5i-YUhoEhYz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "z_1 & = W_1 x + b_1 \\\\\n",
        "a_1 & = \\tanh(z_1) \\\\\n",
        "z_2 & = W_2 a_1 + b_2 \\\\\n",
        "a_2 & = \\hat{y} = i.e. \\mathrm{softmax}(z_2)\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "zXXXAnE8hbQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $z_i$ is the input of layer $i$\n",
        "- $a_i$ is the output of layer $i$ after applying the activation function (e.g., tanh for layer 1, softmax for layer 2)\n",
        "- $W_1, b_1, W_2, b_2$ are  the weights (parameters) of the network, which we need to learn from our training data. \n",
        "\n",
        "\n",
        "##### Softmax:\n",
        "The softmax activation function gives out normalized class probabilities: it takes a vector of real-valued scores (in zz) and squashes it to a vector of values between zero and one that sum to one"
      ],
      "metadata": {
        "id": "47XncwY5hdhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimization\n",
        "\n",
        "The amount of error is calculated through the *loss function* (loss for prediction with respect to true labels). The loss function can be considered as a quality measure and is given by:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "i0sjNXX8hf3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this function does is to sum over our training examples and add to the loss if the prediction was incorrect. \n",
        "\n",
        "So, **the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be.** "
      ],
      "metadata": {
        "id": "OKcN_ldfhiOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the parameters that minimize our loss function we use a **gradient descent$^{1}$** to find its minimum, that is we repeatedly evaluate the gradient and then perform a parameter update.\n",
        "\n",
        "The gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step.\n",
        "\n",
        "To keep things simple we are going to use a batch gradient descent with a fixed learning rate. \n",
        "\n",
        "As an input, gradient descent needs the gradients (a vector of slopes or the derivatives) of the loss function with respect to the parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. \n",
        "\n",
        "[*The derivative of a function $y = f(x)$ of a variable $x$ is a measure of the rate at which the value $y$ of the function changes with respect to the change of the variable x. It describes the instantaneous rate of change of the variable.*]\n",
        "\n",
        "\n",
        "To calculate these gradients we use the *backpropagation algorithm*, an efficient way to calculate the gradients starting from the output. \n",
        "\n",
        "Applying the backpropagation formula we find the following:"
      ],
      "metadata": {
        "id": "FpZInoeQhk0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "& \\delta_3 = y - \\hat{y} \\\\\n",
        "& \\delta_2 = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_3\\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_1}} = x^T \\delta_2\\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_2 \\\\\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "0UgiiafThnbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "x3j4eJ3yhrR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization parameters\n",
        "num_examples = len(X) # Size of the training set\n",
        "nn_input_dim = 2      # Dimensionality of the input layer\n",
        "nn_output_dim = 2     # Dimensionality of the output layer\n",
        "\n",
        "# Gradient descent parameters \n",
        "epsilon = 0.01 # Learning rate for the gradient descent\n",
        "reg_lambda = 0.01 # Regularization strength"
      ],
      "metadata": {
        "id": "h6J8weWphwZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a loss function evaluating the performance of the model we are creating:"
      ],
      "metadata": {
        "id": "nIElsxdZht9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def calculate_loss(model):\n",
        "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
        "    # Forward propagation to calculate the predictions\n",
        "    z1 = X.dot(W1) + b1\n",
        "    a1 = ## YOUR CODE HERE // hint: look for numpy function \"tanh()\"\n",
        "    z2 = ## YOUR CODE HERE // hint: take a look at z1\n",
        "    # Now we compute a2 based on the softmax function, implemented like this:\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    \n",
        "    # Calculate the loss\n",
        "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
        "    data_loss = np.sum(corect_logprobs)\n",
        "    \n",
        "    # Add regulatization term to loss.\n",
        "    # We encode some preference for a certain set of weights W over others to remove the ambiguity\n",
        "    # given by the fact that other solutions could be possible (i.e., we discourage large weights \n",
        "    # through an element-wise quadratic penalty over all parameters)\n",
        "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "    return 1./num_examples * data_loss"
      ],
      "metadata": {
        "id": "Acdz-pcLhzmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement a helper function to calculate the output of the network. \n",
        "\n",
        "It does forward propagation as defined above and returns the class with the highest probability."
      ],
      "metadata": {
        "id": "ya6mvHwUh16i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the output (0 or 1)\n",
        "def predict(model, x):\n",
        "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
        "    # Forward propagation\n",
        "    z1 = ## YOUR CODE HERE\n",
        "    a1 = ## YOUR CODE HERE\n",
        "    z2 = ## YOUR CODE HERE\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    return np.argmax(probs, axis=1)"
      ],
      "metadata": {
        "id": "h0x6UW2ch2C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NN model \n",
        "\n",
        "Here we define the function that learns the parameters of the network through the batch gradient descent mechanism described above. "
      ],
      "metadata": {
        "id": "4JjYwcdph9DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - nn_hdim: Number of nodes in the hidden layer\n",
        "# - num_passes: Number of passes through the training data for gradient descent\n",
        "# - print_loss: If True, print the loss every 1000 iterations\n",
        "\n",
        "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
        "    \n",
        "    # 1. We initialize the weights to learn to random values\n",
        "    np.random.seed(0)\n",
        "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
        "    b2 = np.zeros((1, nn_output_dim))\n",
        "\n",
        "    # We initialize our result\n",
        "    model = {}\n",
        "    \n",
        "    # Gradient descent.\n",
        "    for i in range(0, num_passes):\n",
        "\n",
        "        # Forward propagation\n",
        "        z1 = X.dot(W1) + b1\n",
        "        a1 = np.tanh(z1)\n",
        "        z2 = a1.dot(W2) + b2\n",
        "        exp_scores = np.exp(z2)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # Backpropagation\n",
        "        delta3 = probs\n",
        "        delta3[range(num_examples), y] -= 1\n",
        "        dW2 = ## YOUR CODE HERE // hint: take a look at the formulas above. Also, you can compute the transpose of a matrix like this: x.T, where \"x\" is your matrix\n",
        "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
        "        dW1 = ## YOUR CODE HERE // hint: same as with dW2\n",
        "        db1 = np.sum(delta2, axis=0)\n",
        "\n",
        "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
        "        dW2 += reg_lambda * W2\n",
        "        dW1 += reg_lambda * W1\n",
        "\n",
        "        # Gradient descent parameter update\n",
        "        W1 += -epsilon * dW1\n",
        "        b1 += -epsilon * db1\n",
        "        W2 += -epsilon * dW2\n",
        "        b2 += -epsilon * db2\n",
        "        \n",
        "        # Assign new parameters to the model\n",
        "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "        \n",
        "        # Print the loss every 1000 iterations.\n",
        "        if print_loss and i % 1000 == 0:\n",
        "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "V_x67GdXh96p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A network with a hidden layer of size 3\n",
        "\n",
        "We train our a network with a hidden layer made of 3 units."
      ],
      "metadata": {
        "id": "xrkNqj_IiO9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with a 3-dimensional hidden layer\n",
        "model = build_model(3, print_loss=True)"
      ],
      "metadata": {
        "id": "HLaOayIriPs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is the result?"
      ],
      "metadata": {
        "id": "4ngJIbL4gntf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundary\n",
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size 3\");\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q5_JQ1F3ij7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Varying the hidden layer size\n",
        "What happens if we vary the number of hidden units?"
      ],
      "metadata": {
        "id": "WZkD53dMiqNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n",
        "\n",
        "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer size %d' % nn_hdim)\n",
        "    model = build_model(nn_hdim)\n",
        "    plot_decision_boundary(lambda x: predict(model, x))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BqcGoOKHimlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A hidden layer of low dimensionality captures very well the general trend of our data, but higher dimensionalities tend to overfit the data.\n",
        "\n",
        "#### Generalization problem!!!\n",
        "\n",
        "The network memorizes the training set but is not able to correctly classify a different set of data from a similar dataset"
      ],
      "metadata": {
        "id": "yrS3w7IIix7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NEXT: Use the 'tensorflow playground' interactive tool to get a deeper understanding of how these neural networks perform under different conditions: https://playground.tensorflow.org"
      ],
      "metadata": {
        "id": "hqIy2w022ooJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: Applying MLP with `scikit-learn`"
      ],
      "metadata": {
        "id": "XcsCHwUfygOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Build a MLP that correctly classifies handwritten digits receiving as input the raw pixels from scanned images of digits. As an input to our network we will use the intensities of the image pixels. The dataset we are going to use contains a series of 8x8 greyscale images."
      ],
      "metadata": {
        "id": "S8TJcTLnEC2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are all the modules you will need to build and train your network\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = load_digits()"
      ],
      "metadata": {
        "id": "Pg8AfPGryrOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First take a look at the content of the dataset"
      ],
      "metadata": {
        "id": "DtOjWBAjEHjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(digits.keys())"
      ],
      "metadata": {
        "id": "vIyoA8yAEHqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `images` attribute contains the 8x8 images \n",
        "* The `data` attribute contains the features that can be used to classify the digits (flattened version of the `images` attribute)\n",
        "* The `target_names` contains the names of the classes\n",
        "* The `DESCR` attribute stores a description of the dataset\n",
        "* The `target` attribute provides the ground truth of the dataset (the correct class)\n",
        "\n",
        "You can read the description accessing the `DESCR` variable:"
      ],
      "metadata": {
        "id": "m8jcr7EpELn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(digits['DESCR'])"
      ],
      "metadata": {
        "id": "qOlO3WOKERJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* How many instances are contained in the dataset?\n",
        "* How many attributes?\n",
        "* How many classes?\n",
        "\n",
        "but it's better to verify the real content of the dataset by yourself"
      ],
      "metadata": {
        "id": "kFmiVm81ETfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nr of instances (they are far less than what is reported in the DESCRiption)\n",
        "n_instances = len(digits.images)\n",
        "print(n_instances)\n",
        "\n",
        "n_instances = np.shape(digits.data)[0]\n",
        "print(n_instances)\n",
        "\n",
        "# Nr of features\n",
        "n_attributes = np.shape(digits.data)[1]\n",
        "print(n_attributes)\n",
        "\n",
        "# Nr of classes\n",
        "n_classes = len(digits.target_names)\n",
        "print(n_classes)"
      ],
      "metadata": {
        "id": "_fGNEaOIEX49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "for index, (image, label) in enumerate(images_and_labels[4:8]):\n",
        "    plt.subplot(2, 4, index + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('Training: %i' % label)"
      ],
      "metadata": {
        "id": "BtGsc2X-EcDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input units: flattening the dataset\n",
        "The input layer of your MLP contains units encoding values of the input pixels.\n",
        "To apply the MLP classifier to your data, you need to flatten the images, that is you convert each 8x8 matrix to a 64 units vector. (NB. The dataset already contains a flattened version of the data (`data`) but here we will use the `images` instead).\n",
        "\n",
        "You can use the `reshape` function to make the conversion (check the online help if you don't know how the `reshape` function works)."
      ],
      "metadata": {
        "id": "NMnPjs4JV2yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = digits.images.reshape(( n_instances, -1 ))"
      ],
      "metadata": {
        "id": "euyRb3EuEcvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "6G52xLSXV8X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Divide the data into a training and a test set"
      ],
      "metadata": {
        "id": "XX9wTdIAV-CD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and test sets. This can be done using the `train_test_split` function from the `model_selection` module.\n",
        "\n",
        "[Hint: you can set the size of your test dataset through `test_size` in the range 0-1]"
      ],
      "metadata": {
        "id": "AxdSYYxCghCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_test_split?"
      ],
      "metadata": {
        "id": "vN2EWP_CguGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data, digits.target)"
      ],
      "metadata": {
        "id": "al8KDonxV-Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the size of your training and test sets"
      ],
      "metadata": {
        "id": "ekmzDXusgzV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training', X_train.shape)\n",
        "print('Test', X_test.shape)"
      ],
      "metadata": {
        "id": "qwKyMokWgzdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scale your dataset"
      ],
      "metadata": {
        "id": "4y_Bi39EWD1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP is sensitive to feature scaling, so it is highly recommended that you scale your data (i.e., having mean 0 and std 1).\n",
        "\n",
        "Apply some scaling (normalization) to your data to improve the convergence of the network to a solution."
      ],
      "metadata": {
        "id": "8_mriq7veCkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a new scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Prepare the scaler based on the training data\n",
        "scaler.fit(X_train)"
      ],
      "metadata": {
        "id": "dnqacTJLWEJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0,:]"
      ],
      "metadata": {
        "id": "HYpyUdTXI1N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now apply the transformations to your data\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "iRRNyVQJWEMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0,:]"
      ],
      "metadata": {
        "id": "9yMNpY9pWEP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train your model\n",
        "With `scikit-learn` this becomes a very easy process. At first we import the MultiLayer Perceptron Classifier model from the `neural-network` library."
      ],
      "metadata": {
        "id": "uRq3AUf7hZuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "6fO4nikRhiDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an instance of the network. At this stage you can define different parameters of your model (to know more how the function works just type `MLPClassifier?` for its documentation)"
      ],
      "metadata": {
        "id": "epP_6umMeWOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLPClassifier?"
      ],
      "metadata": {
        "id": "TEDgJA8ieW-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are quite some parameters that you can set. For this example we are just going to define the size of the hidden layers and leave the rest to the default values.\n",
        "I.e.:\n",
        "* `activation` : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
        "    Activation function for the hidden layer.\n",
        "\n",
        "    * 'identity', no-op activation,\n",
        "      returns f(x) = x\n",
        "\n",
        "    * 'logistic', the logistic sigmoid function,\n",
        "      returns f(x) = 1 / (1 + exp(-x)).\n",
        "\n",
        "    * 'tanh', the hyperbolic tan function,\n",
        "      returns f(x) = tanh(x).\n",
        "\n",
        "    * 'relu', the rectified linear unit function,\n",
        "      returns f(x) = max(0, x)\n",
        "      \n",
        "* `solver`: the solver for weight optimization (i.e., gradient descent)\n",
        "\n",
        "* `learning_rate` : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
        "    Learning rate schedule for weight updates."
      ],
      "metadata": {
        "id": "hv4NcwseejEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)"
      ],
      "metadata": {
        "id": "2ujNQZnCWO_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learn your training set (fit your data)"
      ],
      "metadata": {
        "id": "r9lTAVSTWTrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "coOthe_mWTyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test your model on the remaining data"
      ],
      "metadata": {
        "id": "-LorFscUWT5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = mlp.predict(X_test)"
      ],
      "metadata": {
        "id": "OdzMGRZGWUAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Pred\\t True')\n",
        "for i in range(20):\n",
        "    print('%d\\t %d' % (predictions[i], y_test[i]))"
      ],
      "metadata": {
        "id": "v3w8Hz6hWcBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### And check the results\n",
        "\n",
        "Create a classification report and a confusion matrix"
      ],
      "metadata": {
        "id": "m6_HBs9sWgDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_report = classification_report(y_test, predictions)\n",
        "\n",
        "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
        "      % (mlp, c_report))\n",
        "\n",
        "cfn_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(\"Confusion matrix:\\n%s\" % cfn_matrix)"
      ],
      "metadata": {
        "id": "_1opXq_6WgNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the confusion matrix\n"
      ],
      "metadata": {
        "id": "toPQxLpEWjS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, \"{:1.2f}\".format(cm[i, j]),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "mubXiSkqYmH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plot_confusion_matrix(cfn_matrix, classes=digits.target_names, normalize=True)"
      ],
      "metadata": {
        "id": "bjMF9e-eWjaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the first four examples of the test set and the their predicted label"
      ],
      "metadata": {
        "id": "qpkst4mvWvzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_and_predictions = list(zip(np.reshape(X_test,(450,8,-1)), predictions))\n",
        "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
        "    plt.subplot(2, 4, index + 5)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.title('Predict.: %i' % prediction)"
      ],
      "metadata": {
        "id": "35JivSWzWv5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the weights of the MLP \n",
        "In general it is not easy to make sense of the coefficients generated by a MLP, but for some tasks looking at the learned coefficients can provide insights into the learning behavior of the network. The plot below shows for each hidden unit what is the configuration of weights that makes the unit activate."
      ],
      "metadata": {
        "id": "vSyRbSnlW1EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15,6))\n",
        "im1 = ax.matshow(np.transpose(mlp.coefs_[1]), cmap=plt.get_cmap(\"gray\"))\n",
        "cax = fig.add_axes([.92, .2, .01, .6])\n",
        "fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "ax.set_xlabel('Input Layer', fontsize=18)\n",
        "ax.set_ylabel('Output Layer', fontsize=18)\n",
        "ax.set_title('Weights matrix', fontsize=18)"
      ],
      "metadata": {
        "id": "SdZtnQdTW1NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can reshape the input weights back into the original 8x8 form of the input images and plot the resulting image."
      ],
      "metadata": {
        "id": "n5UxdNYuXECl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(5,2, figsize=(10,6))\n",
        "\n",
        "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
        "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
        "    ax.imshow(coef.reshape(8, 8), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
        "               vmax=.5 * vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.suptitle('Hidden units', fontsize=18)"
      ],
      "metadata": {
        "id": "btE2PA3nW6tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, for the sixth hidden  neuron (plot below) what are the digits you would expect will make it fire?"
      ],
      "metadata": {
        "id": "YLh7pXBwXE7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = np.transpose(mlp.coefs_[0])[3]  # Pull weightings on inputs to the 2nd neuron in the first hidden layer\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(5,5))\n",
        "ax.imshow(np.reshape(hidden, (8,8)), cmap=plt.get_cmap(\"gray\"), vmin=.5 * vmin, vmax=.5 * vmax, aspect=\"auto\")\n",
        "ax.set_title('Hidden unit weights', fontsize=14)"
      ],
      "metadata": {
        "id": "PLEHZj5yXFB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-parameters optimization\n",
        "\n",
        "A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data.\n",
        "\n",
        "A cross-validation generator splits the whole dataset *k* times in training\n",
        "and test data. Subsets of the training set with varying sizes will be used\n",
        "to train the estimator and a score for each training subset size and the\n",
        "test set will be computed. Afterwards, the scores will be averaged over\n",
        "all *k* runs for each training subset size."
      ],
      "metadata": {
        "id": "8oBgH4KJiC97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Split and shuffle your training set in different training and test sets for cross validation\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "\n",
        "# create an instance of your classifier\n",
        "mlp2 = MLPClassifier(hidden_layer_sizes=(10,))\n",
        "\n",
        "# Set the different sizes for the training data\n",
        "train_sizes = np.linspace(.1, 1.0, 5)\n",
        "\n",
        "# Run the crossvalidation\n",
        "train_sizes, train_scores, test_scores = learning_curve(mlp2, X_train, y_train, \n",
        "                                            cv=cv, train_sizes=train_sizes, n_jobs=-1)\n",
        "\n",
        "# Calculate statistics\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure()\n",
        "plt.grid()\n",
        "plt.title(\"Learning curves\")\n",
        "plt.xlabel(\"Training examples\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "plt.legend(loc=\"best\")"
      ],
      "metadata": {
        "id": "tQCmv6k6iDGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is sometimes useful plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.\n",
        "The `model-selection` package offers an additional function to test your model which is called `validation-curve`.\n",
        "If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well."
      ],
      "metadata": {
        "id": "dnCAdGF-iX_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise:** Now try to use the `MLPClassifier` to identify breast cancer based on a real dataset!"
      ],
      "metadata": {
        "id": "me7rZtHpfPf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the 'cancer' dataset from the avialable datasets"
      ],
      "metadata": {
        "id": "Tt0W7UxNfvAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "print(data)"
      ],
      "metadata": {
        "id": "YuXcDTu_Y5Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the content of the object you just created"
      ],
      "metadata": {
        "id": "xpFhhSocf02S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "id": "K0n64N6vf3Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the description of the dataset"
      ],
      "metadata": {
        "id": "We8BHeedgOu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['DESCR'])"
      ],
      "metadata": {
        "id": "-fNYJnVegO1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and test sets by using the `train_test_split` function, just as before.\n",
        "\n",
        "**Hint**: data['data'] contains the input data, whereas data['target'] contains the classification labels."
      ],
      "metadata": {
        "id": "dHmXfDUsgUb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = ## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "xAtNmHAvgUiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the size of your training and test sets"
      ],
      "metadata": {
        "id": "llvfyvnEjaTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training', X_train.shape)\n",
        "print('Test', X_test.shape)"
      ],
      "metadata": {
        "id": "4DK9u8iLjacM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale your data using `StandardScaler()`, just as before"
      ],
      "metadata": {
        "id": "taJrkDojjfE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Q-e8W9dCLne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Prepare the Scaler based on the training data\n",
        "## YOUR CODE HERE\n",
        "\n",
        "# Now apply the transformations to your data\n",
        "\n",
        "X_train = ## YOUR CODE HERE\n",
        "X_test  = ## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "2CMHZ0YJjfLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the dataset in a 3D space using the first 3 features"
      ],
      "metadata": {
        "id": "j6ZPA480kBzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(figsize=(7,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(X_train[:,10], X_train[:,11], X_train[:,12], c=y_train, marker='o')\n",
        "\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_zlabel('Feature 3')\n",
        "\n",
        "ax.view_init(30, 120)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2igCkNofkB6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train your `MLPClassifier` model\n",
        "For instance, set it at 20 units per layer, and two hidden layers"
      ],
      "metadata": {
        "id": "lybcFo2pk2B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = ## YOUR CODE HERE // Also, set the maximum number of iterations at 1000\n",
        "\n",
        "# Call the proper function to train your MLP\n",
        "## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "-pLZAY9Pk2JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your model on the remaining data"
      ],
      "metadata": {
        "id": "_TcKG_GBlUpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = ## YOUR CODE HERE"
      ],
      "metadata": {
        "id": "8tfhwdtwlO9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "id": "VSF4z_ublbfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the results"
      ],
      "metadata": {
        "id": "zVWcCTExlgOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure(figsize=(14,6))\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "\n",
        "ax.scatter(X_test[:,0], X_test[:,1], X_test[:,2], c=y_test, marker='o', s=40)\n",
        "\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_zlabel('Feature 3')\n",
        "ax.set_title('Test data')\n",
        "ax.view_init(30, 120)\n",
        "\n",
        "ax1 = fig.add_subplot(122, projection='3d')\n",
        "ax1.scatter(X_test[:,0], X_test[:,1], X_test[:,2], c=predictions+y_test, marker='o', s=40)\n",
        "\n",
        "ax1.set_xlabel('Feature 1')\n",
        "ax1.set_ylabel('Feature 2')\n",
        "ax1.set_zlabel('Feature 3')\n",
        "ax1.set_title('Predicted data (Misclassified examples in green)')\n",
        "\n",
        "ax1.view_init(30, 120)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uyY_Y8Wqlg3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the confusion matrix"
      ],
      "metadata": {
        "id": "L_x9zOyNlkkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=['WDBC-Malignant', 'WDBC-Benign'], normalize=True)"
      ],
      "metadata": {
        "id": "QJkUC8xrllhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, predictions, target_names=['WDBC-Malignant', 'WDBC-Benign']))"
      ],
      "metadata": {
        "id": "fjonsx7RltRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-QFfvPpDgHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}